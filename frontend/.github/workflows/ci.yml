# FolioFox CI/CD Pipeline
# Comprehensive testing and deployment workflow

name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
          - accessibility
      deploy_environment:
        description: 'Environment to deploy to'
        required: false
        default: 'none'
        type: choice
        options:
          - none
          - staging
          - production

env:
  NODE_VERSION: '18'
  CACHE_VERSION: 'v1'

jobs:
  # Code quality and security checks
  quality-checks:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run ESLint
        run: npm run lint -- --format=@microsoft/eslint-formatter-sarif --output-file=eslint-results.sarif
        continue-on-error: true

      - name: Upload ESLint results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: eslint-results.sarif

      - name: Run TypeScript check
        run: npm run type-check

      - name: Check code formatting
        run: npm run format -- --check

      - name: Run security audit
        run: npm audit --audit-level=moderate

      - name: Dependency vulnerability scan
        uses: ossf/scorecard-action@v2.3.1
        with:
          results_file: scorecard-results.sarif
          results_format: sarif
          repo_token: ${{ secrets.SCORECARD_TOKEN }}

      - name: Upload Scorecard results
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: scorecard-results.sarif

  # Unit tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: quality-checks

    strategy:
      matrix:
        node-version: [16, 18, 20]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests
        run: npm run test:unit -- --coverage --reporter=verbose --reporter=junit --outputFile=test-results/unit-results.xml
        env:
          CI: true

      - name: Upload test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Unit Tests (Node ${{ matrix.node-version }})
          path: test-results/unit-results.xml
          reporter: java-junit

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: coverage/lcov.info
          flags: unit
          name: unit-tests-node-${{ matrix.node-version }}

  # Integration tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: quality-checks

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpassword
          POSTGRES_DB: foliofox_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run database migrations
        run: npm run db:migrate
        env:
          DATABASE_URL: postgresql://postgres:testpassword@localhost:5432/foliofox_test
          REDIS_URL: redis://localhost:6379

      - name: Run integration tests
        run: npm run test:integration -- --coverage --reporter=verbose --reporter=junit --outputFile=test-results/integration-results.xml
        env:
          CI: true
          DATABASE_URL: postgresql://postgres:testpassword@localhost:5432/foliofox_test
          REDIS_URL: redis://localhost:6379

      - name: Upload test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Integration Tests
          path: test-results/integration-results.xml
          reporter: java-junit

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: coverage/lcov.info
          flags: integration
          name: integration-tests

  # End-to-end tests
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [unit-tests, integration-tests]

    strategy:
      matrix:
        browser: [chromium, firefox, webkit]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps ${{ matrix.browser }}

      - name: Build application
        run: npm run build

      - name: Run E2E tests
        run: npx playwright test --project=${{ matrix.browser }} --reporter=html --reporter=junit
        env:
          CI: true
          PLAYWRIGHT_JUNIT_OUTPUT_FILE: test-results/e2e-${{ matrix.browser }}-results.xml

      - name: Upload test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: E2E Tests (${{ matrix.browser }})
          path: test-results/e2e-${{ matrix.browser }}-results.xml
          reporter: java-junit

      - name: Upload Playwright report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-report-${{ matrix.browser }}
          path: playwright-report/
          retention-days: 30

      - name: Upload test screenshots
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: test-screenshots-${{ matrix.browser }}
          path: test-results/
          retention-days: 30

  # Performance tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: quality-checks
    if: github.event_name == 'push' || github.event.inputs.test_suite == 'performance' || github.event.inputs.test_suite == 'all'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run performance tests
        run: npm run test:performance -- --reporter=verbose --reporter=junit --outputFile=test-results/performance-results.xml
        env:
          CI: true

      - name: Upload performance results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Performance Tests
          path: test-results/performance-results.xml
          reporter: java-junit

      - name: Performance regression check
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name == 'push'
        with:
          tool: 'customSmallerIsBetter'
          output-file-path: test-results/performance-benchmark.json
          external-data-json-path: ./cache/benchmark-data.json
          fail-on-alert: true
          alert-threshold: '200%'
          comment-on-alert: true
          github-token: ${{ secrets.GITHUB_TOKEN }}

  # Accessibility tests
  accessibility-tests:
    name: Accessibility Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: quality-checks

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps chromium

      - name: Build application
        run: npm run build

      - name: Run accessibility tests
        run: npm run test:accessibility -- --reporter=html --reporter=junit
        env:
          CI: true
          PLAYWRIGHT_JUNIT_OUTPUT_FILE: test-results/accessibility-results.xml

      - name: Upload accessibility results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Accessibility Tests
          path: test-results/accessibility-results.xml
          reporter: java-junit

      - name: Upload accessibility report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: accessibility-report
          path: playwright-report/
          retention-days: 30

  # Security tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: quality-checks

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Run CodeQL analysis
        uses: github/codeql-action/init@v3
        with:
          languages: javascript
          queries: security-and-quality

      - name: Perform CodeQL analysis
        uses: github/codeql-action/analyze@v3

  # Build and artifact generation
  build:
    name: Build Application
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [unit-tests, integration-tests]

    outputs:
      version: ${{ steps.version.outputs.version }}
      build-hash: ${{ steps.build.outputs.hash }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Generate version
        id: version
        run: |
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            VERSION="pr-${{ github.event.number }}-$(git rev-parse --short HEAD)"
          elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            VERSION="$(date +%Y%m%d)-$(git rev-parse --short HEAD)"
          else
            VERSION="$(git rev-parse --abbrev-ref HEAD)-$(git rev-parse --short HEAD)"
          fi
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Building version: $VERSION"

      - name: Build application
        id: build
        run: |
          npm run build
          BUILD_HASH=$(find dist -type f -exec sha256sum {} \; | sha256sum | cut -d' ' -f1)
          echo "hash=$BUILD_HASH" >> $GITHUB_OUTPUT
          echo "Build hash: $BUILD_HASH"
        env:
          VITE_VERSION: ${{ steps.version.outputs.version }}
          VITE_BUILD_TIME: ${{ github.run_id }}

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-${{ steps.version.outputs.version }}
          path: dist/
          retention-days: 90

      - name: Generate build manifest
        run: |
          cat > build-manifest.json << EOF
          {
            "version": "${{ steps.version.outputs.version }}",
            "hash": "${{ steps.build.outputs.hash }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit": "${{ github.sha }}",
            "branch": "${{ github.ref_name }}",
            "workflow_run": "${{ github.run_id }}"
          }
          EOF

      - name: Upload build manifest
        uses: actions/upload-artifact@v4
        with:
          name: build-manifest
          path: build-manifest.json

  # Docker image build and security scan
  docker-build:
    name: Docker Build & Scan
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: build
    if: github.event_name == 'push'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: build-${{ needs.build.outputs.version }}
          path: dist/

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: Dockerfile
          push: false
          tags: |
            ghcr.io/${{ github.repository }}:${{ needs.build.outputs.version }}
            ghcr.io/${{ github.repository }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
          outputs: type=docker,dest=/tmp/foliofox-image.tar

      - name: Scan Docker image for vulnerabilities
        uses: aquasecurity/trivy-action@master
        with:
          input: '/tmp/foliofox-image.tar'
          format: 'sarif'
          output: 'docker-trivy-results.sarif'

      - name: Upload Docker scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'docker-trivy-results.sarif'

      - name: Push Docker image
        if: github.ref == 'refs/heads/main'
        run: |
          docker load -i /tmp/foliofox-image.tar
          docker push ghcr.io/${{ github.repository }}:${{ needs.build.outputs.version }}
          docker push ghcr.io/${{ github.repository }}:latest

  # Deployment to staging
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [build, e2e-tests, docker-build]
    if: github.ref == 'refs/heads/develop' || github.event.inputs.deploy_environment == 'staging'
    environment: staging

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy to staging
        run: |
          echo "Deploying version ${{ needs.build.outputs.version }} to staging..."
          # Add your deployment logic here
          # e.g., kubectl, helm, terraform, etc.

      - name: Run smoke tests
        run: |
          echo "Running smoke tests against staging..."
          # Add smoke test commands here

      - name: Notify deployment
        uses: 8398a7/action-slack@v3
        if: always()
        with:
          status: ${{ job.status }}
          channel: '#deployments'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          fields: repo,message,commit,author,action,eventName,ref,workflow

  # Deployment to production
  deploy-production:
    name: Deploy to Production  
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [build, e2e-tests, docker-build, deploy-staging]
    if: github.ref == 'refs/heads/main' || github.event.inputs.deploy_environment == 'production'
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy to production
        run: |
          echo "Deploying version ${{ needs.build.outputs.version }} to production..."
          # Add your production deployment logic here

      - name: Run production smoke tests
        run: |
          echo "Running smoke tests against production..."
          # Add production smoke test commands here

      - name: Create GitHub release
        if: github.ref == 'refs/heads/main'
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: v${{ needs.build.outputs.version }}
          release_name: Release v${{ needs.build.outputs.version }}
          body: |
            ## Changes
            Auto-generated release for version ${{ needs.build.outputs.version }}
            
            **Build Hash:** ${{ needs.build.outputs.build-hash }}
            **Commit:** ${{ github.sha }}
            
            ## Test Results
            - ✅ Unit Tests
            - ✅ Integration Tests  
            - ✅ E2E Tests
            - ✅ Accessibility Tests
            - ✅ Security Scans
          draft: false
          prerelease: false

      - name: Notify production deployment
        uses: 8398a7/action-slack@v3
        if: always()
        with:
          status: ${{ job.status }}
          channel: '#releases'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          fields: repo,message,commit,author,action,eventName,ref,workflow

  # Test results summary
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    if: always()
    needs: [quality-checks, unit-tests, integration-tests, e2e-tests, performance-tests, accessibility-tests, security-tests]

    steps:
      - name: Download all test results
        uses: actions/download-artifact@v4

      - name: Generate test summary
        run: |
          echo "# Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Quality Checks | ${{ needs.quality-checks.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | ${{ needs.unit-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.integration-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| E2E Tests | ${{ needs.e2e-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Tests | ${{ needs.performance-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Accessibility Tests | ${{ needs.accessibility-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Security Tests | ${{ needs.security-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run:** [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY